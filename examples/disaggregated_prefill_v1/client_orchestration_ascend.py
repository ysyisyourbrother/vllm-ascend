from openai import OpenAI
import uuid
import time
import threading
import requests
import asyncio
import concurrent.futures
from datetime import datetime
import os
import subprocess

# Ascend NPUç¯å¢ƒä¸‹çš„DPåè°ƒå™¨KVç¼“å­˜é•¿åº¦ç»Ÿè®¡åŠŸèƒ½é›†æˆæµ‹è¯•å®¢æˆ·ç«¯
# DP Coordinator KV Cache Length Statistics Integration Test Client for Ascend NPU
#
# æµ‹è¯•ç›®æ ‡ï¼šéªŒè¯KVç¼“å­˜é•¿åº¦ç»Ÿè®¡ä¿¡æ¯èƒ½å¤Ÿä»æ‰€æœ‰DPå¼•æ“æ­£ç¡®æ”¶é›†åˆ°è´Ÿè½½å‡è¡¡å™¨
# Test Goal: Verify that KV cache length statistics can be correctly collected
# from all DP engines to the load balancer
#
# é€‚é…è¯´æ˜ï¼š
# - ç›´æ¥è¿æ¥åˆ°Ascend NPUä¸Šçš„prefillerå’Œdecoderå®ä¾‹
# - ä¸ä¾èµ–proxy serverï¼Œç›´æ¥é€šè¿‡clientè°ƒç”¨è¿›è¡Œæµ‹è¯•
# - æ”¯æŒAscend NPUç¯å¢ƒçš„ç‰¹å®šé…ç½®

# å…¨å±€å˜é‡ç”¨äºç›‘æ§ç»Ÿè®¡
monitoring_active = True
stats_history = []
log_monitoring_active = True

def monitor_kv_cache_stats():
    """ç›‘æ§KVç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„åå°çº¿ç¨‹å‡½æ•°"""
    print("ğŸ” å¼€å§‹ç›‘æ§KVç¼“å­˜ç»Ÿè®¡ä¿¡æ¯...")

    while monitoring_active:
        time.sleep(5)  # æ¯5ç§’ç›‘æ§ä¸€æ¬¡

def monitor_log_files():
    """å®æ—¶ç›‘æ§æ—¥å¿—æ–‡ä»¶ä¸­çš„KVç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    print("ğŸ“‹ å¼€å§‹å®æ—¶ç›‘æ§æ—¥å¿—æ–‡ä»¶ä¸­çš„KVç¼“å­˜ç»Ÿè®¡ä¿¡æ¯...")

    log_files = ["decode_dp.log", "prefill_dp.log"]
    last_positions = {log_file: 0 for log_file in log_files}

    while log_monitoring_active:
        for log_file in log_files:
            if os.path.exists(log_file):
                try:
                    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                        f.seek(last_positions[log_file])
                        new_lines = f.readlines()
                        last_positions[log_file] = f.tell()

                        for line in new_lines:
                            # åªæ˜¾ç¤ºDPLBAsyncMPClientçš„å…³é”®ç›‘æ§ä¿¡æ¯
                            if any(keyword in line for keyword in [
                                'ğŸ¯ DPLBAsyncMPClientç›‘æ§æ›´æ–°', 'âš ï¸ DPLBAsyncMPClientç›‘æ§æ›´æ–°'
                            ]):
                                timestamp = datetime.now().strftime("%H:%M:%S")
                                print(f"ğŸ“‹ [{timestamp}] {line.strip()}")

                except Exception as e:
                    # å¿½ç•¥æ–‡ä»¶è¯»å–é”™è¯¯ï¼Œç»§ç»­ç›‘æ§
                    pass

        time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡æ—¥å¿—æ–‡ä»¶

def create_dynamic_prompt(base_prompt, target_tokens):
    """æ ¹æ®ç›®æ ‡tokenæ•°åŠ¨æ€ç”Ÿæˆprompt"""
    # ä¼°ç®—å½“å‰promptçš„tokenæ•°ï¼ˆä¸­æ–‡çº¦3å­—ç¬¦/tokenï¼‰
    current_tokens = len(base_prompt) // 3

    if current_tokens >= target_tokens:
        return base_prompt

    # éœ€è¦æ‰©å±•çš„tokenæ•°
    tokens_needed = target_tokens - current_tokens
    chars_needed = tokens_needed * 3

    # æ‰©å±•å†…å®¹æ¨¡æ¿ - æ›´é•¿çš„å†…å®¹
    extensions = [
        "è¯·åœ¨å›ç­”ä¸­åŒ…å«å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚å’Œå®ç°æ–¹æ³•ï¼Œè¯¦ç»†è¯´æ˜æ¯ä¸ªæ­¥éª¤çš„æ“ä½œæµç¨‹å’Œå…³é”®å‚æ•°è®¾ç½®ã€‚",
        "åŒæ—¶æä¾›ç›¸å…³çš„ä»£ç ç¤ºä¾‹å’Œæœ€ä½³å®è·µï¼ŒåŒ…æ‹¬å®Œæ•´çš„å®ç°ä»£ç ã€é…ç½®æ–‡ä»¶ç¤ºä¾‹å’Œè¿è¡Œç»“æœåˆ†æã€‚",
        "åˆ†æè¯¥æŠ€æœ¯åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°å’Œä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬è®¡ç®—å¤æ‚åº¦åˆ†æã€å†…å­˜ä½¿ç”¨ä¼˜åŒ–ã€å¹¶å‘å¤„ç†èƒ½åŠ›ç­‰æ–¹é¢çš„è¯¦ç»†è¯„ä¼°ã€‚",
        "è®¨è®ºè¯¥é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•å’Œæœªæ¥å‘å±•è¶‹åŠ¿ï¼Œå¼•ç”¨è¿‘æœŸçš„é‡è¦å­¦æœ¯è®ºæ–‡å’Œå·¥ä¸šç•Œçš„åˆ›æ–°å®è·µæ¡ˆä¾‹ã€‚",
        "å¯¹æ¯”ä¸åŒæŠ€æœ¯æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨æ¡ä»¶ï¼Œä»æ€§èƒ½ã€å¯æ‰©å±•æ€§ã€ç»´æŠ¤æˆæœ¬ã€å­¦ä¹ æ›²çº¿ç­‰å¤šä¸ªç»´åº¦è¿›è¡Œå…¨é¢æ¯”è¾ƒã€‚",
        "è¯´æ˜åœ¨å®é™…é¡¹ç›®ä¸­çš„éƒ¨ç½²ç»éªŒå’Œæ³¨æ„äº‹é¡¹ï¼ŒåŒ…æ‹¬ç¯å¢ƒé…ç½®ã€ä¾èµ–ç®¡ç†ã€æ€§èƒ½è°ƒä¼˜ã€æ•…éšœæ’é™¤ç­‰å®è·µç»éªŒã€‚",
        "æä¾›è¯¦ç»†çš„æ•°å­¦å…¬å¼æ¨å¯¼å’Œç†è®ºåˆ†æï¼ŒåŒ…æ‹¬ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦åˆ†æï¼Œä»¥åŠæ”¶æ•›æ€§è¯æ˜ã€‚",
        "ä¸¾ä¾‹è¯´æ˜åœ¨å·¥ä¸šç•Œçš„æˆåŠŸåº”ç”¨æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬å…·ä½“çš„ä¸šåŠ¡åœºæ™¯ã€æŠ€æœ¯æ¶æ„ã€å®æ–½è¿‡ç¨‹å’Œå–å¾—çš„æ•ˆæœã€‚",
        "åˆ†æå¯èƒ½é‡åˆ°çš„æŠ€æœ¯æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¸¸è§çš„é”™è¯¯ç±»å‹ã€è°ƒè¯•æ–¹æ³•ã€æ€§èƒ½ç“¶é¢ˆè¯†åˆ«å’Œä¼˜åŒ–ç­–ç•¥ã€‚",
        "è®¨è®ºè¯¥æŠ€æœ¯å¯¹ç›¸å…³é¢†åŸŸçš„å½±å“å’Œæ„ä¹‰ï¼Œåˆ†æå…¶åœ¨æ¨åŠ¨è¡Œä¸šå‘å±•å’ŒæŠ€æœ¯åˆ›æ–°æ–¹é¢çš„é‡è¦ä½œç”¨ã€‚",
        "è¯·ç»“åˆæœ€æ–°çš„å­¦æœ¯è®ºæ–‡å’Œç ”ç©¶æˆæœè¿›è¡Œåˆ†æï¼Œå¼•ç”¨æƒå¨çš„æ•°æ®å’Œå®éªŒç»“æœæ¥æ”¯æŒä½ çš„è§‚ç‚¹ã€‚",
        "è¯´æ˜è¯¥æŠ€æœ¯çš„å‘å±•å†ç¨‹å’Œå…³é”®é‡Œç¨‹ç¢‘ï¼Œæ¢³ç†ä»æ—©æœŸæ¦‚å¿µåˆ°ç°åœ¨æˆç†Ÿåº”ç”¨çš„æ¼”è¿›è¿‡ç¨‹ã€‚",
        "åˆ†æä¸åŒå‚æ•°è®¾ç½®å¯¹ç»“æœçš„å½±å“ï¼Œæä¾›å‚æ•°è°ƒä¼˜çš„æŒ‡å¯¼åŸåˆ™å’Œç»éªŒæ€»ç»“ã€‚",
        "è®¨è®ºè¯¥æŠ€æœ¯åœ¨ä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å°è§„æ¨¡ã€ä¸­ç­‰è§„æ¨¡å’Œå¤§è§„æ¨¡æ•°æ®é›†çš„å¤„ç†èƒ½åŠ›å¯¹æ¯”ã€‚",
        "æä¾›æ€§èƒ½è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ç»“æœï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ã€å¤„ç†é€Ÿåº¦ç­‰å…³é”®æŒ‡æ ‡çš„è¯¦ç»†åˆ†æã€‚",
        "è¯´æ˜ä¸å…¶ä»–ç›¸å…³æŠ€æœ¯çš„é›†æˆæ–¹æ³•ï¼ŒåŒ…æ‹¬APIæ¥å£è®¾è®¡ã€æ•°æ®æ ¼å¼è½¬æ¢ã€ç³»ç»Ÿæ¶æ„æ•´åˆç­‰æŠ€æœ¯ç»†èŠ‚ã€‚",
        "åˆ†æè®¡ç®—å¤æ‚åº¦å’Œèµ„æºæ¶ˆè€—æƒ…å†µï¼ŒåŒ…æ‹¬CPUä½¿ç”¨ç‡ã€å†…å­˜å ç”¨ã€ç½‘ç»œå¸¦å®½éœ€æ±‚ç­‰èµ„æºæ¶ˆè€—çš„è¯¦ç»†åˆ†æã€‚",
        "è®¨è®ºå¯æ‰©å±•æ€§å’Œå¹¶è¡ŒåŒ–å®ç°æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ°´å¹³æ‰©å±•ã€å‚ç›´æ‰©å±•ã€åˆ†å¸ƒå¼éƒ¨ç½²ç­‰ä¸åŒæ‰©å±•ç­–ç•¥çš„æ¯”è¾ƒã€‚",
        "æä¾›æ•…éšœæ’é™¤å’Œè°ƒè¯•æŠ€å·§ï¼ŒåŒ…æ‹¬æ—¥å¿—åˆ†æã€æ€§èƒ½ç›‘æ§ã€é”™è¯¯è¯Šæ–­ç­‰å®ç”¨çš„è¿ç»´ç»éªŒã€‚",
        "è¯´æ˜æ ‡å‡†åŒ–å’Œè§„èŒƒåŒ–çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬ä»£ç è§„èŒƒã€æ–‡æ¡£æ ‡å‡†ã€æµ‹è¯•è§„èŒƒç­‰è½¯ä»¶å·¥ç¨‹æœ€ä½³å®è·µã€‚"
    ]

    # æ›´è¯¦ç»†çš„æ‰©å±•å†…å®¹
    detailed_extensions = [
        "è¯·ç¡®ä¿å›ç­”å†…å®¹ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¸¥å¯†ï¼ŒåŒ…å«å¼•è¨€ã€ä¸»ä½“å’Œç»“è®ºéƒ¨åˆ†ã€‚åœ¨å¼•è¨€éƒ¨åˆ†ç®€è¦ä»‹ç»èƒŒæ™¯å’Œç›®æ ‡ï¼Œåœ¨ä¸»ä½“éƒ¨åˆ†è¯¦ç»†é˜è¿°æ ¸å¿ƒå†…å®¹ï¼Œåœ¨ç»“è®ºéƒ¨åˆ†æ€»ç»“è¦ç‚¹å’Œå±•æœ›æœªæ¥ã€‚",
        "åœ¨æŠ€æœ¯è§£é‡Šä¸­ä½¿ç”¨å‡†ç¡®çš„ä¸“ä¸šæœ¯è¯­ï¼Œå¹¶æä¾›å¿…è¦çš„èƒŒæ™¯çŸ¥è¯†ã€‚å¯¹äºå¤æ‚çš„æ¦‚å¿µï¼Œè¯·å…ˆç»™å‡ºå®šä¹‰ï¼Œç„¶åé€šè¿‡ç±»æ¯”å’Œç¤ºä¾‹æ¥å¸®åŠ©ç†è§£ã€‚",
        "é€šè¿‡å…·ä½“çš„æ•°æ®å’Œå®éªŒç»“æœæ¥æ”¯æŒä½ çš„è§‚ç‚¹å’Œç»“è®ºã€‚å¼•ç”¨å¯é çš„æ•°æ®æºï¼Œæä¾›è¯¦ç»†çš„å®éªŒè®¾è®¡å’Œç»“æœåˆ†æã€‚",
        "è€ƒè™‘ä¸åŒè¯»è€…çš„æŠ€æœ¯èƒŒæ™¯ï¼Œæä¾›é€‚å½“çš„è§£é‡Šæ·±åº¦ã€‚å¯¹äºåˆå­¦è€…ï¼Œæä¾›æ›´å¤šçš„åŸºç¡€çŸ¥è¯†ï¼›å¯¹äºä¸“å®¶ï¼Œé‡ç‚¹å…³æ³¨é«˜çº§ç‰¹æ€§å’Œæœ€æ–°å‘å±•ã€‚",
        "å¼•ç”¨æƒå¨çš„å­¦æœ¯èµ„æºå’Œè¡Œä¸šæ ‡å‡†æ¥å¢å¼ºå†…å®¹çš„å¯ä¿¡åº¦ã€‚åŒ…æ‹¬é¡¶çº§ä¼šè®®è®ºæ–‡ã€çŸ¥åæœŸåˆŠæ–‡ç« ã€å®˜æ–¹æ–‡æ¡£ç­‰å¯é æ¥æºã€‚",
        "è®¨è®ºè¯¥æŠ€æœ¯çš„å±€é™æ€§å’Œæ”¹è¿›ç©ºé—´ï¼Œä¿æŒå®¢è§‚çš„åˆ†ææ€åº¦ã€‚ä¸ä»…è¦è¯´æ˜ä¼˜ç‚¹ï¼Œä¹Ÿè¦è¯šå®åœ°æŒ‡å‡ºå­˜åœ¨çš„é—®é¢˜å’ŒæŒ‘æˆ˜ã€‚",
        "æä¾›å®ç”¨çš„å»ºè®®å’ŒæŒ‡å¯¼ï¼Œå¸®åŠ©è¯»è€…åœ¨å®é™…å·¥ä½œä¸­åº”ç”¨ç›¸å…³çŸ¥è¯†ã€‚åŒ…æ‹¬å…·ä½“çš„æ“ä½œæ­¥éª¤ã€é…ç½®å‚æ•°ã€æ³¨æ„äº‹é¡¹ç­‰ã€‚",
        "ä½¿ç”¨å›¾è¡¨ã€æµç¨‹å›¾æˆ–ç¤ºä¾‹ä»£ç æ¥è¾…åŠ©è¯´æ˜å¤æ‚çš„æ¦‚å¿µã€‚é€šè¿‡å¯è§†åŒ–çš„æ–¹å¼è®©æŠ½è±¡çš„æ¦‚å¿µæ›´å®¹æ˜“ç†è§£ã€‚",
        "åˆ†æè¯¥æŠ€æœ¯å¯¹è¡Œä¸šå‘å±•å’Œç¤¾ä¼šè¿›æ­¥çš„æ½œåœ¨å½±å“ã€‚ä»ç»æµæ•ˆç›Šã€ç¤¾ä¼šä»·å€¼ã€ç¯å¢ƒå½±å“ç­‰å¤šä¸ªè§’åº¦è¿›è¡Œç»¼åˆè¯„ä¼°ã€‚",
        "è®¨è®ºç›¸å…³çš„ä¼¦ç†é—®é¢˜å’Œç¤¾ä¼šè´£ä»»è€ƒè™‘ã€‚ç‰¹åˆ«æ˜¯åœ¨AIå’Œæ•°æ®å¤„ç†é¢†åŸŸï¼Œè¦å…³æ³¨éšç§ä¿æŠ¤ã€ç®—æ³•å…¬å¹³æ€§ç­‰é‡è¦è®®é¢˜ã€‚"
    ]

    # å¼€å§‹æ‰©å±•prompt
    extended_prompt = base_prompt
    import random

    # é¦–å…ˆæ·»åŠ åŸºç¡€æ‰©å±•
    random.shuffle(extensions)
    for ext in extensions:
        if len(extended_prompt) >= target_tokens * 3:
            break
        extended_prompt += " " + ext

    # å¦‚æœè¿˜ä¸å¤Ÿé•¿ï¼Œæ·»åŠ è¯¦ç»†æ‰©å±•
    if len(extended_prompt) < target_tokens * 3:
        random.shuffle(detailed_extensions)
        for ext in detailed_extensions:
            if len(extended_prompt) >= target_tokens * 3:
                break
            extended_prompt += " " + ext

    # å¦‚æœè¿˜æ˜¯ä¸å¤Ÿé•¿ï¼Œé‡å¤æ·»åŠ å†…å®¹
    while len(extended_prompt) < target_tokens * 3:
        additional_content = "è¯·æä¾›æ›´å¤šçš„æŠ€æœ¯ç»†èŠ‚ã€å®ç°æ–¹æ¡ˆã€åº”ç”¨æ¡ˆä¾‹å’Œæœ€ä½³å®è·µç»éªŒã€‚"
        extended_prompt += " " + additional_content
        if len(additional_content) * 10 > chars_needed:  # é¿å…æ— é™å¾ªç¯
            break

    return extended_prompt

def create_long_tail_prompts():
    """åˆ›å»ºç¬¦åˆé•¿å°¾åˆ†å¸ƒçš„çœŸå®æµ‹è¯•prompté›†åˆ"""
    import random

    # åŸºç¡€promptæ¨¡æ¿
    base_prompts = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·è¯¦ç»†è§£é‡ŠAIçš„å®šä¹‰ã€å‘å±•å†ç¨‹å’Œä¸»è¦åº”ç”¨é¢†åŸŸã€‚",
        "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†å’Œä¸»è¦ç®—æ³•ç±»å‹ã€‚",
        "æ·±åº¦å­¦ä¹ ç›¸æ¯”ä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ ¸å¿ƒç»„ä»¶å’Œåº”ç”¨åœºæ™¯æœ‰å“ªäº›ï¼Ÿ",
        "è¯·ä»‹ç»Transformeræ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°ç‚¹å’ŒæŠ€æœ¯ä¼˜åŠ¿ã€‚",
        "ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿè¯·å…¨é¢è§£é‡Šå…¶æ¦‚å¿µå’Œåº”ç”¨ã€‚",
        "åˆ†å¸ƒå¼è®­ç»ƒåœ¨å¤§è§„æ¨¡æœºå™¨å­¦ä¹ ä¸­çš„é‡è¦æ€§å’Œå®ç°æ–¹æ³•ã€‚",
        "å¤§è¯­è¨€æ¨¡å‹çš„ä¸»è¦ç‰¹ç‚¹å’ŒæŠ€æœ¯æ¶æ„ã€‚",
        "ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†å’ŒåŸºæœ¬ç»„æˆç»“æ„ã€‚",
        "æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ•°å­¦åŸç†å’Œä¼˜åŒ–å˜ç§ã€‚",
        "å·ç§¯ç¥ç»ç½‘ç»œåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ã€‚",
        "å¾ªç¯ç¥ç»ç½‘ç»œå¤„ç†åºåˆ—æ•°æ®çš„æœºåˆ¶ã€‚",
        "å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œç®—æ³•æ¡†æ¶ã€‚",
        "ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å·¥ä½œåŸç†å’Œåº”ç”¨åœºæ™¯ã€‚",
        "è¿ç§»å­¦ä¹ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„é‡è¦ä½œç”¨ã€‚"
    ]

    # æ ¹æ®ä¸åŒé•¿åº¦è¦æ±‚ç”Ÿæˆprompt
    short_prompts = [create_dynamic_prompt(prompt, random.randint(100, 300)) for prompt in base_prompts]
    medium_prompts = [create_dynamic_prompt(prompt, random.randint(500, 1000)) for prompt in base_prompts]
    long_prompts = [create_dynamic_prompt(prompt, random.randint(1000, 3000)) for prompt in base_prompts]

    return short_prompts, medium_prompts, long_prompts

def generate_long_tail_requests(total_requests=100, duration_seconds=10, random_seed=42):
    """
    ç”Ÿæˆç¬¦åˆé•¿å°¾åˆ†å¸ƒçš„çœŸå®æµ‹è¯•è¯·æ±‚åºåˆ—

    Args:
        total_requests: æ€»è¯·æ±‚æ•°
        duration_seconds: è¯·æ±‚åˆ†æ•£æ—¶é—´
        random_seed: éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°

    Returns:
        List of request dictionaries with proper distribution:
        - 80% SHORT requests (100-300 tokens input)
        - 15% MEDIUM requests (500-1000 tokens input)
        - 5% LONG requests (1000-3000 tokens input)
    """
    import random

    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°
    random.seed(random_seed)
    print(f"ğŸ² è®¾ç½®éšæœºç§å­: {random_seed}")

    # ç”Ÿæˆä¸åŒé•¿åº¦çš„promptæ¨¡æ¿
    print("ğŸ“ ç”Ÿæˆpromptæ¨¡æ¿...")
    short_prompts, medium_prompts, long_prompts = create_long_tail_prompts()

    print(f"âœ… Promptæ¨¡æ¿ç”Ÿæˆå®Œæˆ:")
    print(f"   - çŸ­promptæ¨¡æ¿: {len(short_prompts)}ä¸ª")
    print(f"   - ä¸­ç­‰promptæ¨¡æ¿: {len(medium_prompts)}ä¸ª")
    print(f"   - é•¿promptæ¨¡æ¿: {len(long_prompts)}ä¸ª")

    # é¢„å…ˆè®¡ç®—æ¯ç§ç±»å‹çš„è¯·æ±‚æ•°é‡
    expected_short_count = int(total_requests * 0.8)  # 80%
    expected_medium_count = int(total_requests * 0.15)  # 15%
    expected_long_count = total_requests - expected_short_count - expected_medium_count  # å‰©ä½™çš„5%

    print(f"ğŸ“Š é¢„æœŸè¯·æ±‚åˆ†å¸ƒ:")
    print(f"   - çŸ­è¯·æ±‚: {expected_short_count}ä¸ª ({expected_short_count/total_requests*100:.1f}%)")
    print(f"   - ä¸­ç­‰è¯·æ±‚: {expected_medium_count}ä¸ª ({expected_medium_count/total_requests*100:.1f}%)")
    print(f"   - é•¿è¯·æ±‚: {expected_long_count}ä¸ª ({expected_long_count/total_requests*100:.1f}%)")

    # åˆ›å»ºè¯·æ±‚ç±»å‹åˆ—è¡¨ï¼Œç¡®ä¿ç²¾ç¡®çš„åˆ†å¸ƒ
    request_types = (
        ['SHORT'] * expected_short_count +
        ['MEDIUM'] * expected_medium_count +
        ['LONG'] * expected_long_count
    )

    # æ‰“ä¹±è¯·æ±‚ç±»å‹é¡ºåº
    random.shuffle(request_types)

    print(f"ğŸ”€ è¯·æ±‚ç±»å‹åºåˆ—å·²æ‰“ä¹±ï¼Œå¼€å§‹ç”Ÿæˆå…·ä½“è¯·æ±‚...")

    requests = []
    type_counters = {'SHORT': 0, 'MEDIUM': 0, 'LONG': 0}

    for i in range(total_requests):
        request_type = request_types[i]
        type_counters[request_type] += 1

        # æ ¹æ®è¯·æ±‚ç±»å‹é€‰æ‹©å¯¹åº”çš„promptå’Œå‚æ•°
        if request_type == "SHORT":
            # çŸ­è¯·æ±‚: 100-300 tokensè¾“å…¥ï¼Œ100-300 tokensè¾“å‡º
            prompt = random.choice(short_prompts)
            max_tokens = random.randint(100, 300)
            expected_input_range = "100-300"
        elif request_type == "MEDIUM":
            # ä¸­ç­‰è¯·æ±‚: 500-1000 tokensè¾“å…¥ï¼Œ300-500 tokensè¾“å‡º
            prompt = random.choice(medium_prompts)
            max_tokens = random.randint(300, 500)
            expected_input_range = "500-1000"
        else:  # LONG
            # é•¿è¯·æ±‚: 1000-3000 tokensè¾“å…¥ï¼Œ500-1000 tokensè¾“å‡º
            prompt = random.choice(long_prompts)
            max_tokens = random.randint(500, 1000)
            expected_input_range = "1000-3000"

        # è®¡ç®—å®é™…prompté•¿åº¦ï¼ˆä¸­æ–‡çº¦3å­—ç¬¦/tokenï¼‰
        actual_prompt_tokens = len(prompt) // 3

        # ç”Ÿæˆå”¯ä¸€çš„è¯·æ±‚ID
        request_id = f"longtail_{request_type.lower()}_{type_counters[request_type]:03d}_{uuid.uuid4().hex[:6]}"

        # åˆ›å»ºè¯·æ±‚å¯¹è±¡
        request_obj = {
            'prompt': prompt,
            'max_tokens': max_tokens,
            'request_id': request_id,
            'request_type': request_type,
            'estimated_prompt_tokens': actual_prompt_tokens,
            'expected_input_range': expected_input_range
        }

        requests.append(request_obj)

        # æ¯10ä¸ªè¯·æ±‚æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (i + 1) % 10 == 0 or (i + 1) == total_requests:
            print(f"ğŸ“ˆ ç”Ÿæˆè¿›åº¦: {i + 1}/{total_requests} ({(i + 1)/total_requests*100:.1f}%)")

    # éªŒè¯æœ€ç»ˆåˆ†å¸ƒ
    final_short_count = sum(1 for req in requests if req['request_type'] == 'SHORT')
    final_medium_count = sum(1 for req in requests if req['request_type'] == 'MEDIUM')
    final_long_count = sum(1 for req in requests if req['request_type'] == 'LONG')

    print(f"âœ… è¯·æ±‚ç”Ÿæˆå®Œæˆï¼Œæœ€ç»ˆåˆ†å¸ƒéªŒè¯:")
    print(f"   - çŸ­è¯·æ±‚: {final_short_count}ä¸ª ({final_short_count/total_requests*100:.1f}%)")
    print(f"   - ä¸­ç­‰è¯·æ±‚: {final_medium_count}ä¸ª ({final_medium_count/total_requests*100:.1f}%)")
    print(f"   - é•¿è¯·æ±‚: {final_long_count}ä¸ª ({final_long_count/total_requests*100:.1f}%)")

    # éªŒè¯tokené•¿åº¦åˆ†å¸ƒ
    short_tokens = [req['estimated_prompt_tokens'] for req in requests if req['request_type'] == 'SHORT']
    medium_tokens = [req['estimated_prompt_tokens'] for req in requests if req['request_type'] == 'MEDIUM']
    long_tokens = [req['estimated_prompt_tokens'] for req in requests if req['request_type'] == 'LONG']

    if short_tokens:
        print(f"ğŸ“ çŸ­è¯·æ±‚tokené•¿åº¦: {min(short_tokens)}-{max(short_tokens)} (ç›®æ ‡: 100-300)")
    if medium_tokens:
        print(f"ğŸ“ ä¸­ç­‰è¯·æ±‚tokené•¿åº¦: {min(medium_tokens)}-{max(medium_tokens)} (ç›®æ ‡: 500-1000)")
    if long_tokens:
        print(f"ğŸ“ é•¿è¯·æ±‚tokené•¿åº¦: {min(long_tokens)}-{max(long_tokens)} (ç›®æ ‡: 1000-3000)")

    return requests

def process_single_request_pd_separated(model, prompt, request_id, max_tokens):
    """
    å¤„ç†å•ä¸ªè¯·æ±‚å¹¶è®°å½•ç»“æœ - å®ç°æ­£ç¡®çš„PDåˆ†ç¦»æ¶æ„æµç¨‹

    Args:
        model: æ¨¡å‹åç§°
        prompt: è¾“å…¥prompt
        request_id: è¯·æ±‚ID
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

    Returns:
        åŒ…å«è¯·æ±‚ç»“æœçš„å­—å…¸

    æ³¨æ„ï¼šæ­¤å‡½æ•°ç›´æ¥ä½¿ç”¨httpxå‘é€è¯·æ±‚ä»¥æ”¯æŒkv_transfer_paramsä¼ é€’
    """
    start_time = time.time()
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]

    try:
        # ä¼°ç®—promptçš„tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡çº¦2-3å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4-5å­—ç¬¦/tokenï¼‰
        estimated_prompt_tokens = len(prompt) // 3  # ç²—ç•¥ä¼°ç®—
        print(f"ğŸš€ [{timestamp}] è¯·æ±‚ {request_id}: å¼€å§‹PDåˆ†ç¦»æ¶æ„å¤„ç†")
        print(f"   ğŸ“ Prompté•¿åº¦: {len(prompt)}å­—ç¬¦ (ä¼°ç®—~{estimated_prompt_tokens}tokens)")
        print(f"   ğŸ¯ ç›®æ ‡ç”Ÿæˆ: {max_tokens}tokens")
        print(f"   ğŸ“Š é¢„æœŸKVç¼“å­˜é•¿åº¦å˜åŒ–: {estimated_prompt_tokens} â†’ {estimated_prompt_tokens + max_tokens}")

        # é˜¶æ®µ1: å‘é€è¯·æ±‚åˆ°Prefillå®ä¾‹è¿›è¡Œé¢„å¡«å……
        prefill_start = time.time()
        print(f"   ğŸ”„ é˜¶æ®µ1: å‘é€åˆ°Prefillå®ä¾‹ (7.150.11.60:20002)")

        # æ„å»ºprefillè¯·æ±‚ï¼ŒåŒ…å«åˆå§‹çš„kv_transfer_params
        # å‚è€ƒproxy serverçš„å®ç°ï¼šsend_request_to_serviceå‡½æ•°
        prefill_request_data = {
            "model": model,
            "prompt": prompt,
            "max_tokens": 1,  # Prefillé˜¶æ®µåªéœ€è¦1ä¸ªtoken
            "temperature": 0.7,
            "stream": False,
            "kv_transfer_params": {
                "do_remote_decode": True,
                "do_remote_prefill": False,
                "remote_engine_id": None,
                "remote_block_ids": None,
                "remote_host": None,
                "remote_port": None,
                "aborted_request": [],
            }
        }

        # ä½¿ç”¨httpxç›´æ¥å‘é€è¯·æ±‚ä»¥æ”¯æŒkv_transfer_params
        import httpx
        async def send_prefill_request():
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    "http://7.150.11.60:20002/v1/completions",
                    json=prefill_request_data,
                    headers={"X-Request-Id": request_id}
                )
                response.raise_for_status()
                return response.json()

        # ç”±äºæˆ‘ä»¬åœ¨åŒæ­¥å‡½æ•°ä¸­ï¼Œéœ€è¦è¿è¡Œå¼‚æ­¥è¯·æ±‚
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        prefill_response_json = loop.run_until_complete(send_prefill_request())
        prefill_duration = time.time() - prefill_start
        print(f"   âœ… é˜¶æ®µ1å®Œæˆ: Prefillè€—æ—¶ {prefill_duration:.2f}ç§’")

        # ä»prefillå“åº”ä¸­æå–kv_transfer_params
        kv_transfer_params = prefill_response_json.get('kv_transfer_params', {})
        print(f"   ğŸ“¦ æå–KVä¼ è¾“å‚æ•°: {bool(kv_transfer_params)}")

        # é˜¶æ®µ2: å‘é€è¯·æ±‚åˆ°Decodeå®ä¾‹è¿›è¡Œè§£ç 
        decode_start = time.time()
        print(f"   ğŸ”„ é˜¶æ®µ2: å‘é€åˆ°Decodeå®ä¾‹ (7.150.11.60:20012)")

        # æ„å»ºdecodeè¯·æ±‚ï¼ŒåŒ…å«ä»prefillè·å–çš„kv_transfer_params
        decode_request_data = {
            "model": model,
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": 0.7,
            "stream": False
        }

        # å°†prefillé˜¶æ®µè·å–çš„kv_transfer_paramsæ·»åŠ åˆ°decodeè¯·æ±‚ä¸­
        if kv_transfer_params:
            decode_request_data["kv_transfer_params"] = kv_transfer_params
            print(f"   ğŸ”— KVç¼“å­˜ä¼ è¾“å‚æ•°å·²æ·»åŠ åˆ°decodeè¯·æ±‚")

        # å‘é€decodeè¯·æ±‚
        async def send_decode_request():
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    "http://7.150.11.60:20012/v1/completions",
                    json=decode_request_data,
                    headers={"X-Request-Id": request_id}
                )
                response.raise_for_status()
                return response.json()

        decode_response_json = loop.run_until_complete(send_decode_request())
        decode_duration = time.time() - decode_start
        print(f"   âœ… é˜¶æ®µ2å®Œæˆ: Decodeè€—æ—¶ {decode_duration:.2f}ç§’")

        end_time = time.time()
        total_duration = end_time - start_time
        timestamp_end = datetime.now().strftime("%H:%M:%S.%f")[:-3]

        # ä»decodeå“åº”ä¸­æå–ç”Ÿæˆçš„æ–‡æœ¬
        if 'choices' in decode_response_json and len(decode_response_json['choices']) > 0:
            generated_text = decode_response_json['choices'][0].get('text', '')
        else:
            generated_text = ''

        generated_tokens = len(generated_text.split()) if generated_text else 0  # ç²—ç•¥ä¼°ç®—tokenæ•°
        total_kv_cache_tokens = estimated_prompt_tokens + generated_tokens

        print(f"âœ… [{timestamp_end}] è¯·æ±‚ {request_id}: PDåˆ†ç¦»æ¶æ„å¤„ç†å®Œæˆ")
        print(f"   â±ï¸  æ€»è€—æ—¶: {total_duration:.2f}ç§’ (Prefill: {prefill_duration:.2f}s + Decode: {decode_duration:.2f}s)")
        print(f"   ğŸ“Š Tokenç»Ÿè®¡: prompt~{estimated_prompt_tokens}, ç”Ÿæˆ~{generated_tokens}, æ€»è®¡~{total_kv_cache_tokens}")
        print(f"   ğŸ’¾ é¢„æœŸKVç¼“å­˜é•¿åº¦: {total_kv_cache_tokens} tokens")
        print(f"   ğŸ”— KVç¼“å­˜ä¼ è¾“: {'æˆåŠŸ' if kv_transfer_params else 'æœªæ£€æµ‹åˆ°å‚æ•°'}")
        print(f"   ğŸ“ ç”Ÿæˆå†…å®¹: {generated_text[:80]}{'...' if len(generated_text) > 80 else ''}")

        return {
            'request_id': request_id,
            'success': True,
            'duration': total_duration,
            'prefill_duration': prefill_duration,
            'decode_duration': decode_duration,
            'prompt_length': len(prompt),
            'generated_length': len(generated_text),
            'estimated_tokens': generated_tokens,
            'estimated_prompt_tokens': estimated_prompt_tokens,
            'total_kv_cache_tokens': total_kv_cache_tokens,
            'kv_transfer_success': bool(kv_transfer_params)
        }

    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        timestamp_error = datetime.now().strftime("%H:%M:%S.%f")[:-3]

        print(f"âŒ [{timestamp_error}] è¯·æ±‚ {request_id}: PDåˆ†ç¦»æ¶æ„å¤„ç†å¤±è´¥ - {str(e)}")
        return {
            'request_id': request_id,
            'success': False,
            'duration': duration,
            'error': str(e)
        }

try:
    print("ğŸ” å¼€å§‹Ascend NPUç¯å¢ƒä¸‹çš„KVç¼“å­˜æ„ŸçŸ¥è°ƒåº¦ç®—æ³•æµ‹è¯•...")

    # 1: è®¾ç½®prefillå’Œdecodeå®ä¾‹çš„å®¢æˆ·ç«¯ï¼ˆå®ç°æ­£ç¡®çš„PDåˆ†ç¦»æ¶æ„æµç¨‹ï¼‰
    openai_api_key = "EMPTY"  # vLLMä¸éœ€è¦çœŸå®çš„APIå¯†é’¥

    # é€‚é…Ascend NPUç¯å¢ƒï¼šå®ç°æ­£ç¡®çš„PDåˆ†ç¦»æ¶æ„
    # ä½¿ç”¨æ‚¨æä¾›çš„é…ç½®å‚æ•°ï¼š--prefiller-hosts 7.150.11.60 --prefiller-port 20002 --decoder-hosts 7.150.11.60 --decoder-ports 20012

    # Prefillå®¢æˆ·ç«¯ - è´Ÿè´£é¢„å¡«å……é˜¶æ®µ
    prefill_client = OpenAI(
        api_key=openai_api_key,
        base_url="http://7.150.11.60:20002/v1",  # Ascend NPUä¸Šçš„Prefillå®ä¾‹URL
        timeout=120.0  # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥å¤„ç†å¹¶å‘è¯·æ±‚
    )

    # Decodeå®¢æˆ·ç«¯ - è´Ÿè´£è§£ç é˜¶æ®µ
    decode_client = OpenAI(
        api_key=openai_api_key,
        base_url="http://7.150.11.60:20012/v1",  # Ascend NPUä¸Šçš„Decodeå®ä¾‹URL
        timeout=120.0  # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥å¤„ç†å¹¶å‘è¯·æ±‚
    )

    # è·å–æ¨¡å‹åç§°ï¼ˆä»prefillå®ä¾‹è·å–ï¼‰
    models = prefill_client.models.list()
    model = models.data[0].id
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"ğŸ”— è¿æ¥åˆ°Ascend NPU Prefillå®ä¾‹: http://7.150.11.60:20002/v1")
    print(f"ğŸ”— è¿æ¥åˆ°Ascend NPU Decodeå®ä¾‹: http://7.150.11.60:20012/v1")

    # 2: KVç¼“å­˜æ„ŸçŸ¥è°ƒåº¦ç®—æ³•çœŸå®è´Ÿè½½æµ‹è¯•
    print("\n" + "="*80)
    print("ğŸ§ª å¼€å§‹Ascend NPUç¯å¢ƒä¸‹çš„KVç¼“å­˜æ„ŸçŸ¥è°ƒåº¦ç®—æ³•çœŸå®è´Ÿè½½æµ‹è¯•")
    print("ğŸ¯ æµ‹è¯•ç›®æ ‡: éªŒè¯KVç¼“å­˜æ„ŸçŸ¥è°ƒåº¦ç®—æ³•åœ¨Ascend NPUçœŸå®è´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°")
    print("ğŸ“Š è§‚å¯Ÿé‡ç‚¹: è°ƒåº¦å†³ç­–è¿‡ç¨‹ã€å¼•æ“è´Ÿè½½åˆ†å¸ƒã€ç³»ç»Ÿååé‡")
    print("ğŸ”¥ è´Ÿè½½ç‰¹å¾: 100ä¸ªè¯·æ±‚ï¼Œ10 QPSï¼Œé•¿å°¾åˆ†å¸ƒï¼ŒçœŸå®prompté•¿åº¦")
    print("ğŸš€ Ascend NPUé…ç½®: Decoderå®ä¾‹ 7.150.11.60:20012")
    print("="*80)

    # ç”Ÿæˆé•¿å°¾åˆ†å¸ƒçš„æµ‹è¯•è¯·æ±‚ï¼ˆä½¿ç”¨å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°ï¼‰
    total_requests = 100  # æ€»è¯·æ±‚æ•°
    duration_seconds = 10  # è¯·æ±‚åˆ†æ•£åœ¨10ç§’å†…åˆ°è¾¾ï¼Œå¹³å‡QPS=10
    random_seed = 42  # å›ºå®šéšæœºç§å­

    print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆé•¿å°¾åˆ†å¸ƒæµ‹è¯•è¯·æ±‚...")
    requests_to_process = generate_long_tail_requests(total_requests, duration_seconds, random_seed)

    print(f"ğŸ“‹ Ascend NPUçœŸå®è´Ÿè½½æµ‹è¯•é…ç½®:")
    print(f"   - æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"   - è¯·æ±‚åˆ°è¾¾æ—¶é—´: {duration_seconds}ç§’å†…åˆ†æ•£åˆ°è¾¾")
    print(f"   - å¹³å‡QPS: {total_requests/duration_seconds:.1f} (æ¯ç§’{total_requests/duration_seconds:.0f}ä¸ªè¯·æ±‚)")
    print(f"   - è¾“å…¥é•¿åº¦: 80%çŸ­è¯·æ±‚(100-300tokens), 15%ä¸­ç­‰è¯·æ±‚(500-1000tokens), 5%é•¿è¯·æ±‚(1000-3000tokens)")
    print(f"   - è¾“å‡ºé•¿åº¦: çŸ­è¯·æ±‚100-300tokens, ä¸­ç­‰è¯·æ±‚300-500tokens, é•¿è¯·æ±‚500-1000tokens")
    print(f"   - éšæœºç§å­: 42 (ç¡®ä¿å¯å¤ç°)")
    print(f"   - ç›®æ ‡å®ä¾‹: Ascend NPU Decoder 7.150.11.60:20012")

    # ç»Ÿè®¡è¯·æ±‚åˆ†å¸ƒå’Œé¢„æœŸè´Ÿè½½
    short_count = sum(1 for req in requests_to_process if req['request_type'] == 'SHORT')
    medium_count = sum(1 for req in requests_to_process if req['request_type'] == 'MEDIUM')
    long_count = sum(1 for req in requests_to_process if req['request_type'] == 'LONG')

    print(f"ğŸ“Š å®é™…ç”Ÿæˆçš„è¯·æ±‚åˆ†å¸ƒ:")
    print(f"   - çŸ­è¯·æ±‚: {short_count}ä¸ª ({short_count/total_requests*100:.1f}%) - è¾“å…¥100-300tokens, è¾“å‡º100-300tokens")
    print(f"   - ä¸­ç­‰è¯·æ±‚: {medium_count}ä¸ª ({medium_count/total_requests*100:.1f}%) - è¾“å…¥500-1000tokens, è¾“å‡º300-500tokens")
    print(f"   - é•¿è¯·æ±‚: {long_count}ä¸ª ({long_count/total_requests*100:.1f}%) - è¾“å…¥1000-3000tokens, è¾“å‡º500-1000tokens")

    print(f"\nğŸ“Š å‡†å¤‡å‘é€ {len(requests_to_process)} ä¸ªçœŸå®è´Ÿè½½è¯·æ±‚åˆ°Ascend NPU...")

    # ä½¿ç”¨æ–°çš„tokenä¼°ç®—æ–¹å¼
    total_estimated_prompt_tokens = sum(req['estimated_prompt_tokens'] for req in requests_to_process)
    total_estimated_generated_tokens = sum(req['max_tokens'] for req in requests_to_process)
    total_estimated_kv_tokens = total_estimated_prompt_tokens + total_estimated_generated_tokens

    print(f"\nğŸ“ˆ é¢„æœŸè´Ÿè½½ç»Ÿè®¡:")
    print(f"   - æ€»è¾“å…¥tokens: ~{total_estimated_prompt_tokens:,}")
    print(f"   - æ€»è¾“å‡ºtokens: ~{total_estimated_generated_tokens:,}")
    print(f"   - æ€»KVç¼“å­˜tokens: ~{total_estimated_kv_tokens:,}")
    print(f"   - å¹³å‡æ¯è¯·æ±‚KVç¼“å­˜: ~{total_estimated_kv_tokens//total_requests:,} tokens")

    # 3: æ‰§è¡Œæ—¶é—´åˆ†æ•£çš„è¯·æ±‚æµ‹è¯•
    print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œæ—¶é—´åˆ†æ•£çš„é•¿å°¾åˆ†å¸ƒè¯·æ±‚åˆ°Ascend NPU...")
    start_time = datetime.now()

    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œè¯·æ±‚ï¼Œä½†æŒ‰æ—¶é—´åˆ†æ•£å‘é€
    max_workers = min(20, len(requests_to_process))  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_request = {}

        # è¯·æ±‚å·²ç»æŒ‰ç”Ÿæˆé¡ºåºæ’åˆ—ï¼Œæ— éœ€é¢å¤–æ’åº

        print(f"ğŸ“¤ å°†åœ¨{duration_seconds}ç§’å†…åˆ†æ•£å‘é€{len(requests_to_process)}ä¸ªè¯·æ±‚åˆ°Ascend NPU (å¹³å‡QPS: {len(requests_to_process)/duration_seconds:.1f})...")
        print("â³ Ascend NPUçœŸå®è´Ÿè½½æµ‹è¯•è¿›è¡Œä¸­ï¼Œè°ƒåº¦å†³ç­–æ—¥å¿—å°†è®°å½•åœ¨decode_dp.logä¸­...")
        print("ğŸ” å…³é”®ä¿¡æ¯: æ¯ä¸ªè¯·æ±‚çš„è°ƒåº¦å†³ç­–åŸå› ã€é€‰æ‹©çš„å¼•æ“ã€ç³»ç»ŸçŠ¶æ€")

        # åˆ†æ•£å‘é€è¯·æ±‚
        request_start_time = time.time()  # ç”¨äºè®¡ç®—ç›¸å¯¹æ—¶é—´
        for i, req_info in enumerate(requests_to_process):
            # è®¡ç®—ç›¸å¯¹åˆ°è¾¾æ—¶é—´
            relative_arrival_time = (i / len(requests_to_process)) * duration_seconds
            elapsed_time = time.time() - request_start_time
            wait_time = relative_arrival_time - elapsed_time
            if wait_time > 0:
                time.sleep(wait_time)

            # æäº¤è¯·æ±‚ - ä½¿ç”¨PDåˆ†ç¦»æ¶æ„å¤„ç†å‡½æ•°
            future = executor.submit(
                process_single_request_pd_separated,
                model,
                req_info['prompt'],
                req_info['request_id'],
                req_info['max_tokens']
            )
            future_to_request[future] = req_info

            # ç®€åŒ–è¯·æ±‚æäº¤ä¿¡æ¯
            if len(future_to_request) % 10 == 0:  # æ¯10ä¸ªè¯·æ±‚æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                print(f"ğŸ“¨ å·²æäº¤ {len(future_to_request)}/{len(requests_to_process)} ä¸ªè¯·æ±‚åˆ°Ascend NPU...")

        # æ”¶é›†ç»“æœ
        results = []
        completed_count = 0

        for future in concurrent.futures.as_completed(future_to_request):
            req_info = future_to_request[future]
            result = future.result()
            results.append(result)
            completed_count += 1

            # æ¯10ä¸ªè¯·æ±‚æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if completed_count % 10 == 0 or completed_count == len(requests_to_process):
                progress = (completed_count / len(requests_to_process)) * 100
                print(f"ğŸ“ˆ è¿›åº¦: {completed_count}/{len(requests_to_process)} ({progress:.1f}%) å®Œæˆ")

    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()

    print(f"\nğŸ‰ æ‰€æœ‰å¹¶å‘è¯·æ±‚å¤„ç†å®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.2f}ç§’")
    print(f"ğŸš€ Ascend NPUæ€§èƒ½: å¹³å‡ {len(requests_to_process)/total_duration:.2f} QPS")

    # 4: åˆ†ææµ‹è¯•ç»“æœ
    print(f"\nğŸ“Š Ascend NPUæµ‹è¯•ç»“æœåˆ†æ:")
    print("="*60)

    successful_requests = [r for r in results if r['success']]
    failed_requests = [r for r in results if not r['success']]

    print(f"âœ… æˆåŠŸè¯·æ±‚: {len(successful_requests)}/{len(results)}")
    print(f"âŒ å¤±è´¥è¯·æ±‚: {len(failed_requests)}/{len(results)}")

    if successful_requests:
        avg_duration = sum(r['duration'] for r in successful_requests) / len(successful_requests)
        avg_prefill_duration = sum(r.get('prefill_duration', 0) for r in successful_requests) / len(successful_requests)
        avg_decode_duration = sum(r.get('decode_duration', 0) for r in successful_requests) / len(successful_requests)
        total_generated_tokens = sum(r.get('estimated_tokens', 0) for r in successful_requests)
        total_prompt_tokens = sum(r.get('estimated_prompt_tokens', 0) for r in successful_requests)
        total_kv_cache_tokens = sum(r.get('total_kv_cache_tokens', 0) for r in successful_requests)

        # KVç¼“å­˜ä¼ è¾“æˆåŠŸç‡ç»Ÿè®¡
        kv_transfer_success_count = sum(1 for r in successful_requests if r.get('kv_transfer_success', False))
        kv_transfer_success_rate = (kv_transfer_success_count / len(successful_requests)) * 100

        print(f"â±ï¸  å¹³å‡å¤„ç†æ—¶é—´: {avg_duration:.2f}ç§’")
        print(f"   ğŸ”„ å¹³å‡Prefillæ—¶é—´: {avg_prefill_duration:.2f}ç§’")
        print(f"   ğŸ”„ å¹³å‡Decodeæ—¶é—´: {avg_decode_duration:.2f}ç§’")
        print(f"ğŸš€ Ascend NPUååé‡: {len(successful_requests)/total_duration:.2f} è¯·æ±‚/ç§’")
        print(f"ğŸ“Š PDåˆ†ç¦»æ¶æ„æ€§èƒ½åˆ†æ:")
        print(f"   - Prefillé˜¶æ®µå æ¯”: {(avg_prefill_duration/avg_duration)*100:.1f}%")
        print(f"   - Decodeé˜¶æ®µå æ¯”: {(avg_decode_duration/avg_duration)*100:.1f}%")
        print(f"ğŸ”— KVç¼“å­˜ä¼ è¾“ç»Ÿè®¡:")
        print(f"   - KVä¼ è¾“æˆåŠŸç‡: {kv_transfer_success_rate:.1f}% ({kv_transfer_success_count}/{len(successful_requests)})")
        print(f"ğŸ“Š Tokenç»Ÿè®¡æ±‡æ€»:")
        print(f"   - æ€»Prompt tokens: ~{total_prompt_tokens}")
        print(f"   - æ€»ç”Ÿæˆtokens: ~{total_generated_tokens}")
        print(f"   - æ€»KVç¼“å­˜tokens: ~{total_kv_cache_tokens}")

        print(f"\nğŸ“ˆ PDåˆ†ç¦»æ¶æ„è¯¦ç»†ç»“æœ:")
        for result in successful_requests:
            prefill_time = result.get('prefill_duration', 0)
            decode_time = result.get('decode_duration', 0)
            kv_transfer_status = "âœ…" if result.get('kv_transfer_success', False) else "âŒ"
            print(f"   ğŸ”¸ {result['request_id']}: "
                  f"æ€»è®¡{result['duration']:.2f}s (P:{prefill_time:.2f}s + D:{decode_time:.2f}s), "
                  f"prompt:{result['prompt_length']}å­—ç¬¦(~{result.get('estimated_prompt_tokens', 0)}tokens), "
                  f"ç”Ÿæˆ:~{result.get('estimated_tokens', 0)}tokens, "
                  f"KVç¼“å­˜:~{result.get('total_kv_cache_tokens', 0)}tokens, "
                  f"KVä¼ è¾“:{kv_transfer_status}")

    if failed_requests:
        print(f"\nâŒ å¤±è´¥è¯·æ±‚è¯¦æƒ…:")
        for result in failed_requests:
            print(f"   ğŸ”¸ {result['request_id']}: {result.get('error', 'Unknown error')}")

    # 5: ç»§ç»­ç›‘æ§DPåè°ƒå™¨KVç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ” ç»§ç»­ç›‘æ§Ascend NPU DPåè°ƒå™¨KVç¼“å­˜ç»Ÿè®¡ä¿¡æ¯30ç§’...")
    print("ğŸ“‹ é‡ç‚¹è§‚å¯Ÿ:")
    print("   - decode_dp.logä¸­çš„'ğŸ“Š æ”¶åˆ°DPç»Ÿè®¡æ›´æ–°'æ¡ç›®")
    print("   - 'ğŸ’¾ KVç¼“å­˜é•¿åº¦ (æœ¬å®¢æˆ·ç«¯ç®¡ç†çš„å¼•æ“)'çš„æ•°å€¼å˜åŒ–")
    print("   - 'ğŸ“ˆ å…¨å±€æ€»KVç¼“å­˜é•¿åº¦'çš„å¢é•¿å’Œå›è½è¿‡ç¨‹")
    print("   - waiting/runningé˜Ÿåˆ—çš„åŠ¨æ€å˜åŒ–")
    print("   - éªŒè¯lb_engines_tokensæ˜¯å¦æ­£ç¡®æ”¶é›†äº†ç»Ÿè®¡æ•°æ®")
    print("   - Ascend NPUç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°")

    # ä¿æŒç›‘æ§30ç§’
    for i in range(10):
        time.sleep(3)
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"ğŸ“Š [{timestamp}] Ascend NPU DPåè°ƒå™¨ç»Ÿè®¡ç›‘æ§ä¸­... ({i+1}/10)")

    # åœæ­¢ç›‘æ§çº¿ç¨‹
    monitoring_active = False
    log_monitoring_active = False

    print(f"\nğŸ‰ Ascend NPUç¯å¢ƒä¸‹çš„DPåè°ƒå™¨KVç¼“å­˜é•¿åº¦ç»Ÿè®¡åŠŸèƒ½é›†æˆæµ‹è¯•å®Œæˆï¼")
    print("ğŸ“Š è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä¸­çš„DPåè°ƒå™¨KVç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:")
    print("   - decode_dp.log: Decodeå¼•æ“çš„DPåè°ƒå™¨ç»Ÿè®¡")
    print("   - æŸ¥æ‰¾åŒ…å«'æ”¶åˆ°DPç»Ÿè®¡æ›´æ–°'ã€'KVç¼“å­˜é•¿åº¦'ã€'lb_engines_tokens'çš„æ—¥å¿—æ¡ç›®")
    print("   - éªŒè¯KVç¼“å­˜é•¿åº¦ç»Ÿè®¡ä¿¡æ¯æ˜¯å¦ä»DPå¼•æ“æ­£ç¡®æµå‘è´Ÿè½½å‡è¡¡å™¨")
    print("   - è§‚å¯ŸKVç¼“å­˜é•¿åº¦ä»åˆå§‹å€¼å¢é•¿åˆ°å³°å€¼å†å›è½çš„å®Œæ•´è¿‡ç¨‹")
    print("   - ç¡®è®¤Ascend NPUç¯å¢ƒä¸‹çš„æ€§èƒ½å’Œç¨³å®šæ€§è¡¨ç°")

    # ç­‰å¾…æ—¥å¿—ç›‘æ§çº¿ç¨‹ç»“æŸ
    time.sleep(2)

except Exception as e:
    print(f"âŒ Ascend NPUç¯å¢ƒä¸‹çš„DPåè°ƒå™¨KVç¼“å­˜é•¿åº¦ç»Ÿè®¡åŠŸèƒ½é›†æˆæµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    print("è¯·æ£€æŸ¥ä»¥ä¸‹é…ç½®:")
    print("   - Ascend NPUä¸Šçš„decodeå®ä¾‹æ˜¯å¦æ­£åœ¨è¿è¡Œä¸”å¯è®¿é—® (7.150.11.60:20012)")
    print("   - PDåˆ†ç¦»æ¶æ„æ˜¯å¦å·²æ­£ç¡®éƒ¨ç½²å¹¶ä¸”æœåŠ¡æ­£å¸¸è¿è¡Œ")
    print("   - ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼Œé˜²ç«å¢™è®¾ç½®æ˜¯å¦æ­£ç¡®")
    print("   - Ascend NPUç¯å¢ƒé…ç½®æ˜¯å¦å®Œæ•´")
    import traceback
    traceback.print_exc()

    # åœæ­¢ç›‘æ§çº¿ç¨‹
    monitoring_active = False
    log_monitoring_active = False
